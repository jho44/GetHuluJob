{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# adapted from https://pyro.ai/examples/prodlda.html\n",
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import torch\n",
    "import time\n",
    "\n",
    "import math\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "from pyro.infer import SVI, TraceMeanField_ELBO\n",
    "from tqdm import trange\n",
    "\n",
    "#plotting libraries!\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Import the wordcloud library\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Apply to movie set**  \n",
    "**end goal:**  \n",
    "want to  map/group movies (doc id) to topics so we can look up a topic and find related movies or vice versa  \n",
    "**what we have:**  \n",
    "movie descriptions for ~9k movies from netflix titles database  \n",
    "**conversion:**  \n",
    "- our LDA model takes in a matrix of token counts and outputs beta values\n",
    "- need to convert what we have into a matrix of token counts using the CountVectorizer function\n",
    "    - https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start timer\n",
    "start_whole = time.perf_counter()\n",
    "\n",
    "# import user rating data\n",
    "users_df = pd.read_csv(\"./ml_netflix.csv\")\n",
    "eval_df = pd.read_csv(\"./ml_netflix.csv\")\n",
    "eval_df.drop_duplicates('title', inplace=True)\n",
    "eval_df.drop(columns=['userId', 'rating'], inplace=True)\n",
    "\n",
    "num_movies = users_df['movieId'].nunique()\n",
    "num_users = num_users = users_df['userId'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary size: 640\n",
      "Corpus size: torch.Size([1058, 640])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "data = eval_df['description'].to_numpy() \n",
    "# `max_df` removes common words - words with frequency higher than this across all documents\n",
    "# `min_df` removes rare words - words that appear in less than min_df number of docs\n",
    "# note: if min_df is too big, will get error in word cloud (i think because not enough data for all topics)\n",
    "# max min_df i've found that works is 5\n",
    "vectorizer = CountVectorizer(min_df=5, max_df=0.7, stop_words='english') # 1, 0.7\n",
    "# docs is a binary matrix of size M x N where M is number of movies, N is number of words in vocab\n",
    "# docs[i, j] is 1 if movie i's descriptoin has word j and 0 otherwise\n",
    "docs = torch.from_numpy(vectorizer.fit_transform(data).toarray())\n",
    "\n",
    "# creates df with words and indices corresponding to words \n",
    "# used to convert betas back to words in word cloud plots\n",
    "vocab = pd.DataFrame(columns=['word', 'index'])\n",
    "vocab['word'] = vectorizer.get_feature_names()\n",
    "vocab['index'] = vocab.index\n",
    "\n",
    "print('Dictionary size: %d' % len(vocab))\n",
    "print('Corpus size: {}'.format(docs.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting global variables related to LDA computation\n",
    "smoke_test=False\n",
    "seed = 0\n",
    "torch.manual_seed(seed)\n",
    "pyro.set_rng_seed(seed)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "num_topics = 15 if not smoke_test else 3\n",
    "docs = docs.float().to(device)\n",
    "batch_size = 32\n",
    "learning_rate = 1e-3\n",
    "num_epochs = 50 if not smoke_test else 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    # Base class for the encoder net, used in the guide\n",
    "    def __init__(self, vocab_size, num_topics, hidden, dropout):\n",
    "        super().__init__()\n",
    "        self.drop = nn.Dropout(dropout)  # to avoid component collapse\n",
    "        self.fc1 = nn.Linear(vocab_size, hidden)\n",
    "        self.fc2 = nn.Linear(hidden, hidden)\n",
    "        self.fcmu = nn.Linear(hidden, num_topics)\n",
    "        self.fclv = nn.Linear(hidden, num_topics)\n",
    "        # NB: here we set `affine=False` to reduce the number of learning parameters\n",
    "        # See https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html\n",
    "        # for the effect of this flag in BatchNorm1d\n",
    "        self.bnmu = nn.BatchNorm1d(num_topics, affine=False)  # to avoid component collapse\n",
    "        self.bnlv = nn.BatchNorm1d(num_topics, affine=False)  # to avoid component collapse\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        h = F.softplus(self.fc1(inputs))\n",
    "        h = F.softplus(self.fc2(h))\n",
    "        h = self.drop(h)\n",
    "        # Œº and Œ£ are the outputs\n",
    "        logtheta_loc = self.bnmu(self.fcmu(h))\n",
    "        logtheta_logvar = self.bnlv(self.fclv(h))\n",
    "        logtheta_scale = (0.5 * logtheta_logvar).exp()  # Enforces positivity\n",
    "        return logtheta_loc, logtheta_scale\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    # Base class for the decoder net, used in the model\n",
    "    def __init__(self, vocab_size, num_topics, dropout):\n",
    "        super().__init__()\n",
    "        self.beta = nn.Linear(num_topics, vocab_size, bias=False)\n",
    "        self.bn = nn.BatchNorm1d(vocab_size, affine=False)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        inputs = self.drop(inputs)\n",
    "        # the output is œÉ(Œ≤Œ∏)\n",
    "        return F.softmax(self.bn(self.beta(inputs)), dim=1)\n",
    "\n",
    "\n",
    "class ProdLDA(nn.Module):\n",
    "    def __init__(self, vocab_size, num_topics, hidden, dropout):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_topics = num_topics\n",
    "        self.encoder = Encoder(vocab_size, num_topics, hidden, dropout)\n",
    "        self.decoder = Decoder(vocab_size, num_topics, dropout)\n",
    "\n",
    "    def model(self, docs):\n",
    "        pyro.module(\"decoder\", self.decoder)\n",
    "        with pyro.plate(\"documents\", docs.shape[0]):\n",
    "            # Dirichlet prior ùëù(ùúÉ|ùõº) is replaced by a logistic-normal distribution\n",
    "            logtheta_loc = docs.new_zeros((docs.shape[0], self.num_topics))\n",
    "            logtheta_scale = docs.new_ones((docs.shape[0], self.num_topics))\n",
    "            logtheta = pyro.sample(\n",
    "                \"logtheta\", dist.Normal(logtheta_loc, logtheta_scale).to_event(1))\n",
    "            theta = F.softmax(logtheta, -1)\n",
    "\n",
    "            # conditional distribution of ùë§ùëõ is defined as\n",
    "            # ùë§ùëõ|ùõΩ,ùúÉ ~ Categorical(ùúé(ùõΩùúÉ))\n",
    "            count_param = self.decoder(theta)\n",
    "            # Currently, PyTorch Multinomial requires `total_count` to be homogeneous.\n",
    "            # Because the numbers of words across documents can vary,\n",
    "            # we will use the maximum count accross documents here.\n",
    "            # This does not affect the result because Multinomial.log_prob does\n",
    "            # not require `total_count` to evaluate the log probability.\n",
    "            total_count = int(docs.sum(-1).max())\n",
    "            pyro.sample(\n",
    "                'obs',\n",
    "                dist.Multinomial(total_count, count_param),\n",
    "                obs=docs\n",
    "            )\n",
    "\n",
    "    def guide(self, docs):\n",
    "        pyro.module(\"encoder\", self.encoder)\n",
    "        with pyro.plate(\"documents\", docs.shape[0]):\n",
    "            # Dirichlet prior ùëù(ùúÉ|ùõº) is replaced by a logistic-normal distribution,\n",
    "            # where Œº and Œ£ are the encoder network outputs\n",
    "            logtheta_loc, logtheta_scale = self.encoder(docs)\n",
    "            logtheta = pyro.sample(\n",
    "                \"logtheta\", dist.Normal(logtheta_loc, logtheta_scale).to_event(1))\n",
    "\n",
    "    def beta(self):\n",
    "        # beta matrix elements are the weights of the FC layer on the decoder\n",
    "        return self.decoder.beta.weight.cpu().detach().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [00:05<00:00,  9.47it/s, epoch_loss=1.41e+03]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished in 0.0882 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# training \n",
    "pyro.clear_param_store()\n",
    "\n",
    "prodLDA = ProdLDA(\n",
    "    vocab_size=docs.shape[1],\n",
    "    num_topics=num_topics,\n",
    "    hidden=100 if not smoke_test else 10,\n",
    "    dropout=0.2\n",
    ")\n",
    "prodLDA.to(device)\n",
    "\n",
    "optimizer = pyro.optim.Adam({\"lr\": learning_rate})\n",
    "svi = SVI(prodLDA.model, prodLDA.guide, optimizer, loss=TraceMeanField_ELBO())\n",
    "num_batches = int(math.ceil(docs.shape[0] / batch_size)) if not smoke_test else 1\n",
    "\n",
    "start = time.perf_counter()\n",
    "bar = trange(num_epochs)\n",
    "for epoch in bar:\n",
    "    running_loss = 0.0\n",
    "    for i in range(num_batches):\n",
    "        batch_docs = docs[i * batch_size:(i + 1) * batch_size, :]\n",
    "        loss = svi.step(batch_docs)\n",
    "        running_loss += loss / batch_docs.size(0)\n",
    "\n",
    "    bar.set_postfix(epoch_loss='{:.2e}'.format(running_loss))\n",
    "end = time.perf_counter()\n",
    "print(f\"Finished in {(end - start)/60:0.4f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_word_cloud(b, ax, v, n):\n",
    "    sorted_, indices = torch.sort(b, descending=True) # sorts betas by descending order\n",
    "    df = pd.DataFrame(indices[:100].numpy(), columns=['index'])\n",
    "    words = pd.merge(df, v[['index', 'word']],\n",
    "                     how='left', on='index')['word'].values.tolist()\n",
    "    sizes = (sorted_[:100] * 1000).int().numpy().tolist()\n",
    "    freqs = {words[i]: sizes[i] for i in range(len(words))}\n",
    "    wc = WordCloud(background_color=\"white\", width=800, height=500)\n",
    "    wc = wc.generate_from_frequencies(freqs)\n",
    "    ax.set_title('Topic %d' % (n + 1))\n",
    "    ax.imshow(wc, interpolation='bilinear')\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "# beta = prodLDA.beta()\n",
    "# fig, axs = plt.subplots(7, 3, figsize=(14, 24))\n",
    "# for n in range(beta.shape[0]):\n",
    "#     i, j = divmod(n, 3)\n",
    "#     plot_word_cloud(beta[n], axs[i, j], vocab, n)\n",
    "# axs[-1, -1].axis('off');\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**adding topic number and probability of being in topic to netflix db**\n",
    "- for each movie, need to find largest topic probability based on words in docs corpus array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>movieId</th>\n",
       "      <th>Topic</th>\n",
       "      <th>Probability</th>\n",
       "      <th>Doc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Grown Ups</td>\n",
       "      <td>Mourning the loss of their beloved junior high...</td>\n",
       "      <td>682</td>\n",
       "      <td>11</td>\n",
       "      <td>0.2994646</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dark Skies</td>\n",
       "      <td>A familys idyllic suburban life shatters when ...</td>\n",
       "      <td>793</td>\n",
       "      <td>13</td>\n",
       "      <td>0.3682979</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Jaws</td>\n",
       "      <td>When an insatiable great white shark terrorize...</td>\n",
       "      <td>103</td>\n",
       "      <td>12</td>\n",
       "      <td>0.2568434</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Jaws 2</td>\n",
       "      <td>Four years after the last deadly shark attacks...</td>\n",
       "      <td>104</td>\n",
       "      <td>12</td>\n",
       "      <td>0.2568434</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>Jaws: The Revenge</td>\n",
       "      <td>After another deadly shark attack, Ellen Brody...</td>\n",
       "      <td>283</td>\n",
       "      <td>12</td>\n",
       "      <td>0.2568434</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 title                                        description  \\\n",
       "0            Grown Ups  Mourning the loss of their beloved junior high...   \n",
       "1           Dark Skies  A familys idyllic suburban life shatters when ...   \n",
       "2                 Jaws  When an insatiable great white shark terrorize...   \n",
       "5               Jaws 2  Four years after the last deadly shark attacks...   \n",
       "104  Jaws: The Revenge  After another deadly shark attack, Ellen Brody...   \n",
       "\n",
       "     movieId  Topic Probability  Doc  \n",
       "0        682     11   0.2994646    0  \n",
       "1        793     13   0.3682979    1  \n",
       "2        103     12   0.2568434    2  \n",
       "5        104     12   0.2568434    3  \n",
       "104      283     12   0.2568434    4  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# betas is a TxW array where T is number of topics, W is number of words\n",
    "# betas[i, j] gives probability that word j is in topic i\n",
    "betas = prodLDA.beta()\n",
    "# for each movie, find maximum of beta values to assign topic\n",
    "topics = []\n",
    "topic_probs = []\n",
    "for i in range(num_movies):\n",
    "    words_tensor = docs[i]\n",
    "    word_indices = ((words_tensor==1).nonzero(as_tuple=True)[0]) # get indices of words used in movie eval_df[i]\n",
    "    max_probability = 0\n",
    "    topic = None\n",
    "    for word_index in word_indices: # find max topic prob given words in movie i from betas object\n",
    "        max_word_probability = max(betas[:,word_index]).numpy()\n",
    "        if max_word_probability > max_probability:\n",
    "            max_probability = max_word_probability\n",
    "            topic = torch.argmax(betas[:,word_index]).numpy()+1\n",
    "    topics.append(topic)\n",
    "    topic_probs.append(max_probability)\n",
    "eval_df['Topic'] = topics\n",
    "eval_df['Probability'] = topic_probs\n",
    "eval_df['Doc'] = range(num_movies)\n",
    "\n",
    "eval_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_by_storyline(title, df): # per jessica's code\n",
    "    recommended = []\n",
    "    top10_list = []\n",
    "    \n",
    "    title = title.lower()\n",
    "    df['title'] = df['title'].str.lower()\n",
    "    topic_num = df[df['title']==title].Topic.values\n",
    "    if len(topic_num) == 0:\n",
    "        print(title + \" not in database\")\n",
    "        return\n",
    "    doc_num = df[df['title']==title].Doc.values    \n",
    "    \n",
    "    output_df = df[df['Topic']==topic_num[0]].sort_values('Probability', ascending=False).reset_index(drop=True)\n",
    "\n",
    "    index = output_df[output_df['Doc']==doc_num[0]].index[0]\n",
    "    \n",
    "    # return the 10 results with closest probability of belonging to topic `topic_num`\n",
    "    top10_list += list(output_df.iloc[index-5:index].index)\n",
    "    top10_list += list(output_df.iloc[index+1:index+6].index)\n",
    "    \n",
    "    output_df['title'] = output_df['title'].str.title()\n",
    "    for each in top10_list:\n",
    "        recommended.append(output_df.iloc[each].title)\n",
    "        \n",
    "    return recommended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Spartacus',\n",
       " \"Bill Burr: I'M Sorry You Feel That Way\",\n",
       " 'Steve Jobs',\n",
       " 'Patriot Games',\n",
       " 'The River Wild',\n",
       " 'Jaws: The Revenge',\n",
       " 'Jaws 2',\n",
       " 'D√©j√† Vu',\n",
       " 'Tarzan',\n",
       " 'Icarus']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recommend_by_storyline(\"Jaws\", eval_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now, we have a way to recommend movies given a movie, now we move to **evaluation** of our model\n",
    "- **MAP@K**\n",
    "- **MAR@K**\n",
    "- **coverage**: percent of items in the traning data the movie is able to recommend on a test set\n",
    "- **personalization**: dissimilarity (1 - cosine similarity) between user's lists of recommendations\n",
    "\n",
    "using movielens dataset of user ratings in order to evaluate based on our top 10 movies for our model?\n",
    "in small movielens db, we have 1119 possible movies\n",
    "\n",
    "let's say that the movies we recommend to each user are the top 10 in the topic that they have the most\n",
    "5.0 ratings in. if no 5.0 ratings, ..\n",
    "\n",
    "relevant links:  \n",
    "https://towardsdatascience.com/evaluation-metrics-for-recommender-systems-df56c6611093  \n",
    "http://sdsawtelle.github.io/blog/output/mean-average-precision-MAP-for-recommender-systems.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>movieId</th>\n",
       "      <th>userId</th>\n",
       "      <th>rating</th>\n",
       "      <th>Topic</th>\n",
       "      <th>Probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Jaws</td>\n",
       "      <td>When an insatiable great white shark terrorize...</td>\n",
       "      <td>103</td>\n",
       "      <td>486</td>\n",
       "      <td>3.0</td>\n",
       "      <td>12</td>\n",
       "      <td>0.256843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Jaws</td>\n",
       "      <td>When an insatiable great white shark terrorize...</td>\n",
       "      <td>103</td>\n",
       "      <td>495</td>\n",
       "      <td>3.0</td>\n",
       "      <td>12</td>\n",
       "      <td>0.256843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Jaws</td>\n",
       "      <td>When an insatiable great white shark terrorize...</td>\n",
       "      <td>103</td>\n",
       "      <td>515</td>\n",
       "      <td>3.0</td>\n",
       "      <td>12</td>\n",
       "      <td>0.256843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Jaws 2</td>\n",
       "      <td>Four years after the last deadly shark attacks...</td>\n",
       "      <td>104</td>\n",
       "      <td>5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>12</td>\n",
       "      <td>0.256843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Jaws 2</td>\n",
       "      <td>Four years after the last deadly shark attacks...</td>\n",
       "      <td>104</td>\n",
       "      <td>17</td>\n",
       "      <td>2.5</td>\n",
       "      <td>12</td>\n",
       "      <td>0.256843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22826</th>\n",
       "      <td>Zoom</td>\n",
       "      <td>Dragged from civilian life, a former superhero...</td>\n",
       "      <td>514</td>\n",
       "      <td>408</td>\n",
       "      <td>3.0</td>\n",
       "      <td>14</td>\n",
       "      <td>0.349144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22827</th>\n",
       "      <td>Zoom</td>\n",
       "      <td>Dragged from civilian life, a former superhero...</td>\n",
       "      <td>514</td>\n",
       "      <td>504</td>\n",
       "      <td>2.0</td>\n",
       "      <td>14</td>\n",
       "      <td>0.349144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22828</th>\n",
       "      <td>Zoom</td>\n",
       "      <td>Dragged from civilian life, a former superhero...</td>\n",
       "      <td>514</td>\n",
       "      <td>548</td>\n",
       "      <td>5.0</td>\n",
       "      <td>14</td>\n",
       "      <td>0.349144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22829</th>\n",
       "      <td>Zoom</td>\n",
       "      <td>Dragged from civilian life, a former superhero...</td>\n",
       "      <td>514</td>\n",
       "      <td>585</td>\n",
       "      <td>3.0</td>\n",
       "      <td>14</td>\n",
       "      <td>0.349144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22830</th>\n",
       "      <td>Zoom</td>\n",
       "      <td>Dragged from civilian life, a former superhero...</td>\n",
       "      <td>514</td>\n",
       "      <td>601</td>\n",
       "      <td>2.5</td>\n",
       "      <td>14</td>\n",
       "      <td>0.349144</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22582 rows √ó 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        title                                        description  movieId  \\\n",
       "2        Jaws  When an insatiable great white shark terrorize...      103   \n",
       "3        Jaws  When an insatiable great white shark terrorize...      103   \n",
       "4        Jaws  When an insatiable great white shark terrorize...      103   \n",
       "5      Jaws 2  Four years after the last deadly shark attacks...      104   \n",
       "6      Jaws 2  Four years after the last deadly shark attacks...      104   \n",
       "...       ...                                                ...      ...   \n",
       "22826    Zoom  Dragged from civilian life, a former superhero...      514   \n",
       "22827    Zoom  Dragged from civilian life, a former superhero...      514   \n",
       "22828    Zoom  Dragged from civilian life, a former superhero...      514   \n",
       "22829    Zoom  Dragged from civilian life, a former superhero...      514   \n",
       "22830    Zoom  Dragged from civilian life, a former superhero...      514   \n",
       "\n",
       "       userId  rating  Topic  Probability  \n",
       "2         486     3.0     12     0.256843  \n",
       "3         495     3.0     12     0.256843  \n",
       "4         515     3.0     12     0.256843  \n",
       "5           5     4.0     12     0.256843  \n",
       "6          17     2.5     12     0.256843  \n",
       "...       ...     ...    ...          ...  \n",
       "22826     408     3.0     14     0.349144  \n",
       "22827     504     2.0     14     0.349144  \n",
       "22828     548     5.0     14     0.349144  \n",
       "22829     585     3.0     14     0.349144  \n",
       "22830     601     2.5     14     0.349144  \n",
       "\n",
       "[22582 rows x 7 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add topics to users_df\n",
    "users_df['Topic'] = 0\n",
    "users_df.dropna(inplace=True)\n",
    "\n",
    "for index, row in users_df.iterrows():\n",
    "    users_df.at[index, 'Topic'] = eval_df[eval_df['movieId'] == row['movieId']]['Topic'].iloc[0]\n",
    "    users_df.at[index, 'Probability'] = eval_df[eval_df['movieId'] == row['movieId']]['Probability'].iloc[0]\n",
    "\n",
    "users_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each topic, precompute sorted probabilities\n",
    "per_topic = [] # list of dfs\n",
    "\n",
    "for topic in range(1, num_topics+1):\n",
    "    output_df = eval_df[eval_df['Topic']==topic].sort_values('Probability', ascending=False).reset_index(drop=True)\n",
    "    per_topic.append(output_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "605\n"
     ]
    }
   ],
   "source": [
    "# calculate recommendations for each user\n",
    "\"\"\"\n",
    "as suggested in project proposal, number of recommendations k depends on user\n",
    "we set k = minimum(number of movies the user has given 5 stars, number of movies the user has given 4 stars)\n",
    "\n",
    "we calculate recommendation by taking all movies that the user rated 5 stars (or 4 stars if no 5 stars)\n",
    "then, for each movie, we find the topic it is in by finding the topic that it has the highest probability in\n",
    "and find the movie in that topic with probability closest to that movie's topic probabiity\n",
    "\"\"\"\n",
    "\n",
    "# find all 4 and 5 star ratings\n",
    "user_5_df = users_df[users_df['rating']==5]\n",
    "user_4_df = users_df[users_df['rating']==4]\n",
    "\n",
    "# for each user, calculate recommended movies\n",
    "# recommendations is 2d array where recommendations[i] is a list with first value = userId, \n",
    "# subsequent values are k recommended movieIds for the user \n",
    "# where k is number of 5 star ratings given by user and if no 5 star ratings, number of 4 star ratings given by user\n",
    "recommendations = []\n",
    "for user in range(num_users):\n",
    "    # find number of 5 ratings\n",
    "    temp_df = user_5_df[user_5_df['userId']==user]\n",
    "    # if no 5 ratings, find number of 4 ratings\n",
    "    if temp_df.empty:\n",
    "        temp_df = user_4_df[user_4_df['userId']==user]\n",
    "    # if no 4 ratings, go to next user\n",
    "    # if temp_df.empty:\n",
    "    #     continue\n",
    "    recommendations.append([user])\n",
    "    # for each movie\n",
    "    for index, row in temp_df.iterrows():\n",
    "        topic = row['Topic']\n",
    "        movieId = row['movieId']\n",
    "        prob = row['Probability']\n",
    "        topic_df = per_topic[topic-1]\n",
    "        # find closest probability in topic to the current movie\n",
    "        index = topic_df[topic_df['movieId']==movieId].index[0]\n",
    "        # accounting for out of bound accessing\n",
    "        if index == 0:\n",
    "            recommendations[-1].append(row_below['movieId'])\n",
    "            continue\n",
    "        if index == len(topic_df)-1:\n",
    "            recommendations[-1].append(row_above['movieId'])\n",
    "            continue    \n",
    "        row_above = topic_df.iloc[index-1]\n",
    "        row_below = topic_df.iloc[index+1]\n",
    "        if row_below['Probability'] - prob < row_above['Probability'] - prob:\n",
    "            recommendations[-1].append(row_below['movieId'])\n",
    "        else:\n",
    "            recommendations[-1].append(row_above['movieId'])\n",
    "\n",
    "print(len(recommendations))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9221983282499755\n"
     ]
    }
   ],
   "source": [
    "# calculate personalization matrix\n",
    "# https://towardsdatascience.com/evaluation-metrics-for-recommender-systems-df56c6611093\n",
    "# create num_user x num_movies\n",
    "num_users = len(recommendations)\n",
    "personalization = np.zeros((num_users, num_movies))\n",
    "\n",
    "for row in recommendations:\n",
    "    user = row[0]\n",
    "    for movie_ind in row[1:]:\n",
    "        personalization[user][movie_ind] = 1\n",
    "\n",
    "cosine_sim = cosine_similarity(personalization, personalization)\n",
    "\n",
    "# compute average of upper triangle to get cosine similarity\n",
    "sum = 0\n",
    "denom = (num_users-1)*(num_users)/2\n",
    "for i in range(num_users-1):\n",
    "    for j in range(i+1, num_users):\n",
    "        sum += cosine_sim[i][j]\n",
    "\n",
    "similarity = sum/denom\n",
    "dissimilarity = 1 - similarity\n",
    "print(dissimilarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to calculate MAP @ k or MAR @ k\n",
    "# http://sdsawtelle.github.io/blog/output/mean-average-precision-MAP-for-recommender-systems.html\n",
    "\n",
    "# map = true if we wish to calculate precision, false if we wish to calculate recall\n",
    "def map_or_mark(recommendations, num_users, user_5_df, user_4_df, map):\n",
    "    # for each user\n",
    "    map_k = 0\n",
    "    for user in range(num_users):\n",
    "        l = len(recommendations[user])\n",
    "        user_actual = user_5_df[user_5_df['userId']==user]['movieId']\n",
    "        if user_actual.empty:\n",
    "            user_actual = user_4_df[user_4_df['userId']==user]\n",
    "        sum = 0\n",
    "        for k in range(1, l):\n",
    "            divisor = k\n",
    "            if not map:\n",
    "                divisor = float(len(user_actual))\n",
    "            # top k recommendations and top k actually rated movies for user\n",
    "            user_rec = recommendations[user][1:k+1]\n",
    "            user_actual_k = set(user_actual[:k])\n",
    "            # only add if the kth item was relevant\n",
    "            if user_rec[-1] in user_actual_k:\n",
    "                # find intersections \n",
    "                user_rec = set(user_rec)\n",
    "                intersection = list(user_rec & user_actual_k)\n",
    "                # add precision or recall to sum\n",
    "                sum += len(intersection)/divisor\n",
    "        # divide by min(m - number of movies, N)\n",
    "        sum /= float(l)\n",
    "        map_k += sum\n",
    "\n",
    "    # print final map@k value\n",
    "    map_k /= float(num_users)\n",
    "    return map_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP @ k\n",
      "0.0005017060019541162\n",
      "MAR @ k\n",
      "0.00035526841608473226\n"
     ]
    }
   ],
   "source": [
    "# calculate MAP @ k\n",
    "print(\"MAP @ k\")\n",
    "print(map_or_mark(recommendations, num_users, user_5_df, user_4_df, True))\n",
    "\n",
    "# calculate MAR @ k\n",
    "print(\"MAR @ k\")\n",
    "print(map_or_mark(recommendations, num_users, user_5_df, user_4_df, False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished in 0.0882 minutes\n"
     ]
    }
   ],
   "source": [
    "# end timer\n",
    "end_whole = time.perf_counter()\n",
    "print(f\"Finished in {(end - start)/60:0.4f} minutes\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
